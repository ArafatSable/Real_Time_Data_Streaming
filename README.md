# Real-Time Data Streaming

This project demonstrates a real-time â³ data streaming pipeline ğŸ“¦ using **âš’ï¸ Hadoop**, **ğŸ’¡ Kafka**, **âœ¨ Spark**, and **ğŸ“Š MongoDB** to analyze and process ğŸ›’ customer behavior data effectively.

## Features
- **ğŸ’¡ Kafka**: Acts as the message broker ğŸšš to produce and consume ğŸ›’ data streams.
- **âœ¨ Spark**: Processes streaming data in real-time â³ to derive insights and detect âš ï¸ anomalies.
- **ğŸ“Š MongoDB**: Stores processed data for future â² analysis and ğŸ“ˆ reporting.
- **âš’ï¸ Hadoop**: Provides distributed ğŸŒ storage for large-scale ğŸ“¦ data storage and processing.

## Project Structure
```
â”œâ”€â”€ ğŸ›’ E-commerce Customer Behavior.csv           # Sample dataset ğŸ“Š for e-commerce behavior
â”œâ”€â”€ ğŸ“ˆ E-commerce_Customer_Behavior_reports.ipynb # Analysis ğŸ“Š and visualization notebook ğŸ¨
â”œâ”€â”€ ğŸ”„ kafka_consumer_ecommerce_to_console.py     # Kafka ğŸšš consumer script to log data ğŸ”„ to console
â”œâ”€â”€ âš ï¸ kafka_consumer_detect_anamolies.py         # Kafka consumer script for âš ï¸ anomaly detection
â”œâ”€â”€ ğŸ”„ kafka_consumer_ecommerce_to_md.py          # Kafka consumer to process data ğŸ”„ and save to ğŸ“Š MongoDB
â”œâ”€â”€ ğŸšš kafka_producer_ecommerce.py                # Kafka ğŸšš producer script for ğŸ›’ data
â”œâ”€â”€ ğŸ”„ model.py                                   # Contains ğŸ¨ machine learning models for analytics
â”œâ”€â”€ ğŸ¨ Web/                                       # Folder for potential web interface
```

## Prerequisites

1. **ğŸ’¡ Kafka**
   - Install Apache Kafka and ensure ğŸ”§ Zookeeper is running.
   - Create the required âœ¨ topics for the ğŸ›’ data stream.

2. **âš’ï¸ Hadoop**
   - Install and configure âš’ï¸ Hadoop on your system for distributed ğŸŒ data storage.
   - Ensure HDFS is set up and running.

3. **âœ¨ Spark**
   - Install Apache âœ¨ Spark for real-time data â³ processing.
   - Ensure it is integrated with âš’ï¸ Hadoop (if using HDFS).

4. **ğŸ“Š MongoDB**
   - Install ğŸ“Š MongoDB and set up the database ğŸ”§ for storing processed data.

5. **ğŸ”„ Python Libraries**
   Install the required ğŸ¨ Python libraries:
   ```
   pip install kafka-python pyspark pymongo matplotlib pandas numpy
   ```

## Workflow

### 1. Kafka Producer
- **âœ¨ Script**: `kafka_producer_ecommerce.py`
- Reads the `ğŸ›’ E-commerce Customer Behavior.csv` file and sends data to ğŸ”„ Kafka topics.

### 2. Kafka Consumers
- **â³ To Console**: `kafka_consumer_ecommerce_to_console.py` logs data to the console ğŸ”„.
- **To ğŸ“Š MongoDB**: `kafka_consumer_ecommerce_to_md.py` processes and saves data into ğŸ“Š MongoDB.
- **âš ï¸ Anomaly Detection**: `kafka_consumer_detect_anamolies.py` identifies unusual patterns in the ğŸ›’ data stream.

### 3. âœ¨ Spark Processing
- Stream data ğŸ”„ from Kafka topics using âœ¨ Spark Streaming.
- Perform transformations and actions to derive meaningful ğŸ“Š insights.

### 4. ğŸ“Š MongoDB Storage
- Save processed and cleaned data ğŸ”„ for further use and visualization ğŸ¨.

### 5. Analysis and Visualization
- Use the `E-commerce_Customer_Behavior_reports.ipynb` notebook for ğŸ“Š exploratory data analysis (âœ¨ EDA) and visualizations.

## Steps to Run

1. Start ğŸ”§ Zookeeper and Kafka:
   ```bash
   bin/zookeeper-server-start.sh config/zookeeper.properties
   bin/kafka-server-start.sh config/server.properties
   ```
2. Create ğŸ”§ Kafka topics:
   ```bash
   bin/kafka-topics.sh --create --topic ecommerce_data --bootstrap-server localhost:9092
   ```
3. Start the Kafka producer:
   ```bash
   python kafka_producer_ecommerce.py
   ```
4. Start the Kafka consumers:
   ```bash
   python kafka_consumer_ecommerce_to_console.py
   python kafka_consumer_detect_anamolies.py
   python kafka_consumer_ecommerce_to_md.py
   ```
5. Run âœ¨ Spark streaming jobs:
   ```bash
   spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 spark_processing.py
   ```
6. Analyze data in ğŸ“Š MongoDB using tools like MongoDB Compass or integrate with visualization tools.

## Use Cases
- Real-time â³ anomaly detection in ğŸ›’ transactions.
- ğŸ¨ Customer behavior analysis for targeted ğŸŒˆ marketing.
- ğŸ“ˆ Data aggregation and reporting for âœ¨ business intelligence.

## Technologies Used
- **âš’ï¸ Hadoop**: Distributed ğŸŒ storage and processing.
- **ğŸ’¡ Kafka**: Message brokering and real-time data streaming â³.
- **âœ¨ Spark**: Real-time data â³ processing.
- **ğŸ“Š MongoDB**: NoSQL database for storing processed data.
- **ğŸ”„ Python**: Programming language ğŸ”§ for building the pipeline.

## Author
Arafat Sable

Feel free to contribute ğŸŒ or provide suggestions for improvements ğŸ“¢!

